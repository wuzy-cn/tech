{"posts":[{"title":"A Lightweight Microservice Traffic Manager by eBPF","text":"This article explains the design and implementation ideas of a lightweight microservice traffic management project based on eBPF. 本文介绍了一个基于 eBPF 的轻量级微服务流量管理工具的设计和实现思想。 In microservice system, during peak traffic periods, the entire microservice cluster experiences high load. Especially when the load surpasses the cluster’s maximum capacity, it can lead to excessive load on specific nodes, rendering them unable to process subsequent requests. Furthermore, in a microservice system, there are cascading dependencies between service instances. If downstream service instance nodes fail, it can cause upstream service instances to enter a waiting or crashing state. Therefore, there is a need to proactively reject potentially failing requests to achieve quick failure. Currently, the process of fast request failure requires invasive methods like implementing sidecars or using iptables. However, these methods often suffer from performance and operability issues. Hence, there is a need to efficiently determine and redirect request traffic using Linux kernel’s eBPF technology, ensuring that traffic is always directed towards available service instances. As a result, this project leverages eBPF technology in combination with microservice techniques to establish a non-intrusive mechanism for determining and redirecting microservice request traffic. Key capabilities include replacing a large number of iptables lookups and NATs for service requests, supporting fast weighted backend pod selection techniques (instead of random selection), facilitating kernel-level microservice canary testing, and dynamically adjusting the weight assigned to backend pod selection based on external performance metrics (enabling dynamic traffic allocation based on performance metric changes). Overall ArchitectureThe following is the overall architecture of the project: TrafficManager will collect data from multiple sources, including cluster metadata information from Kubernetes, availability and performance data from metric monitoring systems or AI Ops systems. After comprehensive analysis, it will distribute Pod handling and selection logic into kernel-mode Control Map and Data Map. Kernel-mode monitoring and operations begin after attaching the eBPF program. When Pods within the cluster initiate requests to specific services (e.g., http://&lt;svc&gt;.&lt;ns&gt;.svc.cluster.local), the eBPF program attached by TrafficManager intercepts the execution of connect(2) system call. After identifying, analyzing, rendering a verdict, and performing redirection, it completes user-transparent modifications of request, allowing redirection to a new target Pod. At this point, the request will smoothly traverse the overlay network and directly reach the target Pod on the target node (http://&lt;pod&gt;.&lt;svc&gt;.&lt;ns&gt;.svc.cluster.local). Design and ImplementationAbstraction and Storage DesignFor eBPF, when we need to pass user-space data to kernel-space, we must utilize eBPF Map. However, eBPF Maps are mostly key-value pairs, which are not conducive to storing the complex information of Pods, Services, and other entities in their original form. Therefore, it is essential to consider the abstraction of these fields and their mapping relationships. As a result, we have divided this part into two maps based on their functions: Data Map and Control Map. Data MapThe Data Map is solely used to store metadata for backend Pods and is indexed using unique identifiers. It serves as data storage. Control MapThe Control Map is used to swiftly analyze the current cluster’s operational status and select an appropriate result to modify the current request based on pre-defined action rules when a request is detected. In its design, it uses target IP, target port, and an index number for lookups. When the index number is 0, it is typically used to analyze the current service’s status and necessitates a secondary lookup to select a backend Pod. Different behaviors correspond to different formats of the “Options” field to achieve several functionalities within this project. Traffic Control MethodsBased on the introduction above, we can discern the defined data structures. Here is an explanation of how these data structures are utilized. Please note that these usage methods may be changed or expanded as development progresses. For the standard backend selection method based on random lookup, we set the index number to 0 and perform a combined lookup in the Control Map using the target IP and port of the current request. This allows us to determine the number of backend Pods of this Service. For example, for Service 1, as illustrated above, there are two backend Pods. Selection is then done based on a 50% distribution, using a random Pod index as the index number for the lookup. After obtaining the Backend ID, we can look up the destination Pod’s IP and port in Data Map. For the weight-based selection method (as seen in the above diagram for Service 2 - old), the initial steps are the same as random lookup selection, but there is an additional field indicating the selection probability (i.e., weight) of the current Pod. The eBPF program employs an $O(log_2(n))$ complexity algorithm to choose a suitable backend Pod. For services marked for traffic canary (as seen in the above diagram for Service 2 - new), there are additional fields to control the selection of the older version service. The selection process for other Pods is similar to the weight-based selection method. However, if the older version service is chosen as the destination for traffic based on weight, we retrieve relevant information for the older version service from Data Map and perform backend Pod selection through a separate lookup process (as shown in the diagram for Service 2 - old). Dynamic Traffic ManagementWith this project, we can achieve dynamic traffic management to address various cluster states. The diagram below outlines a dynamic traffic management approach based on load metrics. After obtaining load metrics through monitoring tools like Node Exporter, cAdvisor, etc., the data is stored in Prometheus. These metrics will be used to assess the availability of the cluster, nodes, and individual Pods. The assessment can be based on traditional metric calculations or incorporate AI Ops techniques for more sophisticated evaluations. Once the availability of the cluster, nodes, and individual Pods has been calculated, TrafficManager will perform comprehensive assessments and design corresponding strategies for the service. These strategies may include traffic handling methods, identification of non-functioning Pods, and traffic allocation proportions for each Pod. Finally, this information is distributed to the kernel space through eBPF maps and programs for request handling.","link":"/p/ebpf-traffic-manager/"},{"title":"【译文】Google Cloud Run 和 Knative 真的一样吗","text":"本文翻译自 Ahmet Alp Balkan 在 2020 年 3 月发布的博文《Is Google Cloud Run really Knative?》。用以阐述谷歌开源的无服务器引擎 Knative 与其商业版无服务器平台 Google Cloud Run 的关系。本文仅供学习交流，如有侵权将立即删除。 Cloud Run 是谷歌的无服务器平台。它声称实现了 Knative API 及其运行时规约。即可以使用相同的 YAML 清单文件，在 Google Cloud Platform 的基础设施或任何 Kubernetes 集群上运行相同的应用程序。 Knative 在 Kubernetes API 的基础上直接实现了自动扩缩容和网络功能。然而，谷歌 Cloud Run 没有使用Kubernetes，它实际上是在谷歌基础架构上对 Knative API 的“重新实现”。 本文将介绍 Knative API 的部分功能，以及在 Cloud Run 上尚不支持的功能。 同样的格式、运行时和APICloud Run 遵循与 Knative 和 Kubernetes 相同的部署格式：可以通过 OCI 镜像（即所谓的 Docker 镜像）进行部署。 当容器被执行时，它遵循与 Knative Serving 运行时规约 相同的 Cloud Run 容器运行时规约。例如，容器将获得相同的一组环境变量、请求将获得相同的一组 HTTP header。 此外，为了扩展对控制平面的兼容性，Cloud Run REST API 也实现了 Knative Serving 的 API 规范。 在 Cloud Run 上使用 Knative API这是一个简单的 Knative Service 部署文件。可以使用它直接将服务部署到 Cloud Run： 12345678910111213141516171819apiVersion: serving.knative.dev/v1kind: Servicemetadata: name: hello annotations: autoscaling.knative.dev/maxScale: &quot;100&quot;spec: template: spec: containerConcurrency: 50 containers: - image: gcr.io/cloudrun/hello resources: limits: cpu: &quot;1&quot; memory: 512Mi env: - name: LOG_LEVEL value: debug 使用下面的命令将 Knative 服务部署到 Cloud Run（像执行 kubectl apply 一样）： 1gcloud alpha run services replace app.yaml --platform=managed 这个示例有意省略了在 Cloud Run 上不起作用的部分。现在谈一谈被隐藏的部分。 尚不生效的部分（暂未支持）Knative Serving API 的某些部分尚未在 Cloud Run 上实现，或者永远不会实现。这些情况通常源于以下原因之一： Knative API 依赖于 Kubernetes 某些原语的底层实现（但这些实现在 Cloud Run 上不可用） 一些容器参数尚不能被 gVisor 支持 正在等待实现 如果查看 Cloud Run REST API 参考中的 Knative RevisionSpec 类型，并搜索“not supported”，将会发现许多 API 字段尚未支持。 不支持的字段 卷和卷挂载： Knative 目前甚至不允许在 Kubernetes上挂载任意类型的卷，因此以下选项会受限： configMap 卷： Cloud Run 目前没有 ConfigMap 的概念（可以使用环境变量env代替它） secret 卷： Cloud Run 没有 Secret 对象（但 Cloud Run 发布了 Secret Manager，用户可以考虑通过这种方式进行集成 Secret） envFrom： 实际上，这在 Knative 规范中是建议的支持的，但由于缺乏 ConfigMap/Secrets，因此没有实现。需要在清单中指定环境变量作为文字量进行传递。 liveness/readiness 探针： 实际上可以做到，但似乎尚未实现。在 Knative 中还有关于删除探针支持的讨论。 imagePullPolicy： Cloud Run 有一种高效的处理镜像的方式，实际上在新容器启动时并不会从镜像库中拉取镜像。 securityContext： 指定entrypoint的 UID。这可能可以实现。 不支持的 API 规范Knative 在配置服务的许多方面（自动缩放、可见性）上大量使用Kubernetes 注释 和标签。但这些注释在 Knative API 规范中没有定义，因此，Cloud Run 可能会将未实现的注释视为自由格式的键值对并悄悄忽略它们。 autoscaling.knative.dev/minScale：目前还不支持，因为 Cloud Run 在一段时间内处于不活跃状态时总是将应用程序缩减到 0。 2021年4月更新：Cloud Run 现在通过此注释支持实例预热。为了避免冷启动，这些实例保持活跃状态，并以较低的价格计费。 autoscaling.knative.dev/maxScale：支持设置 Cloud Run 上的容器实例的最大数量。 serving.knative.dev/visibility：设置服务是否可以外部访问，目前不支持，因为所有的 Cloud Run 服务都拥有一个公共路由（域名格式：*.run.app）。但可以使用 Cloud IAM 权限绑定进行授权。 相同 API 的不同效果 标签： Cloud Run API 与 Kubernetes 标签不完全兼容。例如，example.com/foo 在 Kubernetes 中是一个有效的，但在 Cloud Run API 中不允许。相反，它支持 GCP 资源的标签格式。 serviceAccountName： 在 Knative 中，这指的是 Kubernetes 服务帐户。但在 Cloud Run 中，它是 GCP 服务帐户电子邮件地址。所以使用语法不同。 资源的 requests 和 limits： 虽然 requests/limits 有助于在 Kubernetes 集群中进行 burst 操作，但 Cloud Run 将使用 gVisor 沙箱中指定的 CPU 和内存规格来提供服务，因此 requests 实际上不会起作用。 CPU限制： Cloud Run 目前仅支持 1 或 2 个 CPU（可能会更改），并且不支持小数的核数规格（例如 500m）。 结论来自译者：Cloud Run 与 Knative 的实现机制并不完全相同，甚至会有 API 层面的不兼容，但在声明文件上又有很大的相似性。 对于一些用户来说，这些差别可能影响程度不高，那就可以使用 gcloud run services replace 或 Terraform 在 Google Cloud Run 上进行自动化部署。 但考虑到对一些特性上的依赖，某些用户仍期望使用 Knative，Google Cloud 仍然提供了 Knative 托管版 Cloud Run for Anthos，用户可以在 GKE 上运行自己的 Knative。","link":"/p/is-google-cloud-run-really-a-knative/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/p/hello-world/"}],"tags":[{"name":"eBPF","slug":"eBPF","link":"/tags/eBPF/"},{"name":"Kubernetes","slug":"Kubernetes","link":"/tags/Kubernetes/"},{"name":"microservice","slug":"microservice","link":"/tags/microservice/"},{"name":"AI Ops","slug":"AI-Ops","link":"/tags/AI-Ops/"},{"name":"serverless","slug":"serverless","link":"/tags/serverless/"},{"name":"Knative","slug":"Knative","link":"/tags/Knative/"}],"categories":[{"name":"Cloud Native","slug":"Cloud-Native","link":"/categories/Cloud-Native/"},{"name":"test","slug":"test","link":"/categories/test/"}],"pages":[{"title":"","text":"if (!Date.now) { Date.now = function now() { return new Date().getTime(); }; } (function(funcName, baseObj) { \"use strict\"; // The public function name defaults to window.docReady // but you can modify the last line of this function to pass in a different object or method name // if you want to put them in a different namespace and those will be used instead of // window.docReady(...) funcName = funcName || \"docReady\"; baseObj = baseObj || window; var readyList = []; var readyFired = false; var readyEventHandlersInstalled = false; // call this when the document is ready // this function protects itself against being called more than once function ready() { if (!readyFired) { // this must be set to true before we start calling callbacks readyFired = true; for (var i = 0; i < readyList.length; i++) { // if a callback here happens to add new ready handlers, // the docReady() function will see that it already fired // and will schedule the callback to run right after // this event loop finishes so all handlers will still execute // in order and no new ones will be added to the readyList // while we are processing the list readyList[i].fn.call(window, readyList[i].ctx); } // allow any closures held by these functions to free readyList = []; } } function readyStateChange() { if ( document.readyState === \"complete\" ) { ready(); } } // This is the one public interface // docReady(fn, context); // the context argument is optional - if present, it will be passed // as an argument to the callback baseObj[funcName] = function(callback, context) { if (typeof callback !== \"function\") { throw new TypeError(\"callback for docReady(fn) must be a function\"); } // if ready has already fired, then just schedule the callback // to fire asynchronously, but right away if (readyFired) { setTimeout(function() {callback(context);}, 1); return; } else { // add the function and context to the list readyList.push({fn: callback, ctx: context}); } // if document already ready to go, schedule the ready function to run // IE only safe when readyState is \"complete\", others safe when readyState is \"interactive\" if (document.readyState === \"complete\" || (!document.attachEvent && document.readyState === \"interactive\")) { setTimeout(ready, 1); } else if (!readyEventHandlersInstalled) { // otherwise if we don't have event handlers installed, install them if (document.addEventListener) { // first choice is DOMContentLoaded event document.addEventListener(\"DOMContentLoaded\", ready, false); // backup is window load event window.addEventListener(\"load\", ready, false); } else { // must be IE document.attachEvent(\"onreadystatechange\", readyStateChange); window.attachEvent(\"onload\", ready); } readyEventHandlersInstalled = true; } } })(\"__sharethis__docReady\", window); // Document.querySelectorAll method // http://ajaxian.com/archives/creating-a-queryselector-for-ie-that-runs-at-native-speed // Needed for: IE7- if (!document.querySelectorAll) { document.querySelectorAll = function(selectors) { var style = document.createElement('style'), elements = [], element; document.documentElement.firstChild.appendChild(style); document._qsa = []; style.styleSheet.cssText = selectors + '{x-qsa:expression(document._qsa && document._qsa.push(this))}'; window.scrollBy(0, 0); style.parentNode.removeChild(style); while (document._qsa.length) { element = document._qsa.shift(); element.style.removeAttribute('x-qsa'); elements.push(element); } document._qsa = null; return elements; }; } // Document.querySelector method // Needed for: IE7- if (!document.querySelector) { document.querySelector = function(selectors) { var elements = document.querySelectorAll(selectors); return (elements.length) ? elements[0] : null; }; } if (!Array.isArray) { Array.isArray = function(arg) { return Object.prototype.toString.call(arg) === '[object Array]'; }; } Array.prototype.indexOf || (Array.prototype.indexOf = function(d, e) { var a; if (null == this) throw new TypeError('\"this\" is null or not defined'); var c = Object(this); var b = c.length >>> 0; if (0 === b) return -1; a = +e || 0; Infinity === Math.abs(a) && (a = 0); if (a >= b) return -1; for (a = Math.max(0","link":"/sharejs/sharejsmain.js"}]}